{"cells":[{"metadata":{},"cell_type":"markdown","source":"# DSL Lab project (BAseline CNN's)\n## Piyush Raj Gupta, Gaurav, Bhushan Chaudhari\n## Mentor - Sujoy Bhattacharya","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom keras.datasets import cifar10\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPooling2D, Dense, Flatten, Dropout\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ncount = 0\nfor dirname, _, filenames in os.walk('/kaggle/input/cifar10/cifar10/train'):\n    for filename in filenames:\n        count = count+1\n        #print(os.path.join(dirname, filename))\nprint(count)\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1 - Baseline (2 CNN + 2 FC)\nThe architecture involves stacking convolutional layers with small 3×3 filters followed by a max pooling layer. Together, these layers form a block, and these blocks can be repeated where the number of filters in each block is increased with the depth of the network such as 32, 128 for the first two blocks of the model. Padding is used on the convolutional layers to ensure the height and width of the output feature maps matches the inputs.\n\nThis defines the feature detector part of the model. This must be coupled with a classifier part of the model that interprets the features and makes a prediction as to which class a given photo belongs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above model must be coupled with a classifier part of the model that interprets the features and makes a prediction as to which class a given photo belongs.\n\nThis can be fixed for each model that we investigate. First, the feature maps output from the feature extraction part of the model must be flattened. We can then interpret them with one or more fully connected layers, and then output a prediction. The output layer must have 10 nodes for the 10 classes and use the softmax activation function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(Flatten())\nmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(10, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model will be optimized using stochastic gradient descent.\n\nWe will use a modest learning rate of 0.001 and a large momentum of 0.9, both of which are good general starting points. The model will optimize the categorical cross entropy loss function required for multi-class classification and will monitor classification accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know that the images are all pre-segmented (e.g. each image contains a single object), that the images all have the same square size of 32×32 pixels, and that the images are color. Therefore, we can load the images and use them for modeling almost immediately.\n\nWe know that the pixel values for each image in the dataset are unsigned integers in the range between no color and full color, or 0 and 255.\n\nWe do not know the best way to scale the pixel values for modeling, but we know that some scaling will be required.\n\nA good starting point is to normalize the pixel values, e.g. rescale them to the range [0,1]. This involves first converting the data type from unsigned integers to floats, then dividing the pixel values by the maximum value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.datasets import cifar10\nfrom keras.utils import np_utils\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n\ny_train = np_utils.to_categorical(y_train, 10)\ny_test = np_utils.to_categorical(y_test, 10)\n\nhistory = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the accuracy graph of training and validation accuracy vs number of epochs to keep a track","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss_train = history.history['accuracy']\nloss_val = history.history['val_accuracy']\nepochs = range(1,26)\nplt.plot(epochs, loss_train[0:25], 'g', label='Training accuracy')\nplt.plot(epochs, loss_val[0:25], 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the loss graph of training and validation loss vs number of epochs to keep a track","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_train = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1,31)\nplt.plot(epochs, loss_train[0:100], 'g', label='Training loss')\nplt.plot(epochs, loss_val[0:100], 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model -2 (4 CNN + 2 FC)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The architecture involves stacking convolutional layers with small 3×3 filters followed by a max pooling layer. Together, these layers form a block, and these blocks can be repeated where the number of filters in each block is increased with the depth of the network such as 32,64,128, 128 for the first four blocks of the model. Padding is used on the convolutional layers to ensure the height and width of the output feature maps matches the inputs.\n\nAbove model must be coupled with a classifier part of the model that interprets the features and makes a prediction as to which class a given photo belongs.\n\nThis can be fixed for each model that we investigate. First, the feature maps output from the feature extraction part of the model must be flattened. We can then interpret them with one or more fully connected layers, and then output a prediction. The output layer must have 10 nodes for the 10 classes and use the softmax activation function.\n\nThe model will be optimized using stochastic gradient descent.\n\nWe will use a modest learning rate of 0.001 and a large momentum of 0.9, both of which are good general starting points. The model will optimize the categorical cross entropy loss function required for multi-class classification and will monitor classification accuracy.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    opt = SGD(lr=0.001, momentum=0.9)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know that the images are all pre-segmented (e.g. each image contains a single object), that the images all have the same square size of 32×32 pixels, and that the images are color. Therefore, we can load the images and use them for modeling almost immediately.\n\nWe know that the pixel values for each image in the dataset are unsigned integers in the range between no color and full color, or 0 and 255.\n\nWe do not know the best way to scale the pixel values for modeling, but we know that some scaling will be required.\n\nA good starting point is to normalize the pixel values, e.g. rescale them to the range [0,1]. This involves first converting the data type from unsigned integers to floats, then dividing the pixel values by the maximum value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.datasets import cifar10\nfrom keras.utils import np_utils\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n\ny_train = np_utils.to_categorical(y_train, 10)\ny_test = np_utils.to_categorical(y_test, 10)\n\nhistory = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the accuracy graph of training and validation accuracy vs number of epochs to keep a track","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss_train = history.history['accuracy']\nloss_val = history.history['val_accuracy']\nepochs = range(1,26)\nplt.plot(epochs, loss_train[0:25], 'g', label='Training accuracy')\nplt.plot(epochs, loss_val[0:25], 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the loss graph of training and validation loss vs number of epochs to keep a track","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_train = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1,31)\nplt.plot(epochs, loss_train[0:100], 'g', label='Training loss')\nplt.plot(epochs, loss_val[0:100], 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model - 3 (Regularisation with 4 CNN + 2 FC)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now that we have established a baseline model, the VGG architecture with three blocks, we can investigate modifications to the model and the training algorithm that seek to improve performance.\n\nWe have used **regularisation tehniques** in this model to improve the accuracy of model.\n\nThere are many regularization techniques we could try, although the nature of the overfitting observed suggests that perhaps early stopping would not be appropriate and that techniques that slow down the rate of convergence might be useful.\n\nWe had look into the effect of both dropout and weight regularization or weight decay.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\nmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='softmax'))\nopt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know that the images are all pre-segmented (e.g. each image contains a single object), that the images all have the same square size of 32×32 pixels, and that the images are color. Therefore, we can load the images and use them for modeling almost immediately.\n\nWe know that the pixel values for each image in the dataset are unsigned integers in the range between no color and full color, or 0 and 255.\n\nWe do not know the best way to scale the pixel values for modeling, but we know that some scaling will be required.\n\nA good starting point is to normalize the pixel values, e.g. rescale them to the range [0,1]. This involves first converting the data type from unsigned integers to floats, then dividing the pixel values by the maximum value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.datasets import cifar10\nfrom keras.utils import np_utils\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n\ny_train = np_utils.to_categorical(y_train, 10)\ny_test = np_utils.to_categorical(y_test, 10)\n\nhistory = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reviewing the learning curve for the model, we can see that overfitting has been addressed. The model converges well for about 40 or 50 epochs, at which point there is no further improvement on the test dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss_train = history.history['accuracy']\nloss_val = history.history['val_accuracy']\nepochs = range(1,26)\nplt.plot(epochs, loss_train[0:25], 'g', label='Training accuracy')\nplt.plot(epochs, loss_val[0:25], 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the loss graph of training and validation loss vs number of epochs to keep a track\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_train = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1,31)\nplt.plot(epochs, loss_train[0:100], 'g', label='Training loss')\nplt.plot(epochs, loss_val[0:100], 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model -4 (Data Augmentation+Regularisation 4CNN + 2FC)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Loading CIFAR 10 dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(trainX, trainy), (testX, testy) = cifar10.load_data()\nfrom matplotlib import pyplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one hot encode target values\ntrainY = tf.keras.utils.to_categorical(trainy)\ntestY = tf.keras.utils.to_categorical(testy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know that the pixel values for each image in the dataset are unsigned integers in the range between no color and full color, or 0 and 255.\n\nWe do not know the best way to scale the pixel values for modeling, but we know that some scaling will be required.\n\nA good starting point is to normalize the pixel values, e.g. rescale them to the range [0,1]. This involves first converting the data type from unsigned integers to floats, then dividing the pixel values by the maximum value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# scale pixels\ndef prep_pixels(train, test):\n\t# convert from integers to floats\n\ttrain_norm = train.astype('float32')\n\ttest_norm = test.astype('float32')\n\t# normalize to range 0-1\n\ttrain_norm = train_norm / 255.0\n\ttest_norm = test_norm / 255.0\n\t# return normalized images\n\treturn train_norm, test_norm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are two key aspects to present: the diagnostics of the learning behavior of the model during training and the estimation of the model performance.\n\nFirst, the diagnostics involve creating a line plot showing model performance on the train and test set during training. These plots are valuable for getting an idea of whether a model is overfitting, underfitting, or has a good fit for the dataset.\n\nWe will create a single figure with two subplots, one for loss and one for accuracy. The blue lines will indicate model performance on the training dataset and orange lines will indicate performance on the hold out test dataset. The summarize_diagnostics() function below creates and shows this plot given the collected training histories. The plot is saved to file, specifically a file with the same name as the script with a ‘png‘ extension.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def summarize_diagnostics(history, filename):\n    import sys\n    # plot loss\n    pyplot.subplot(211)\n    pyplot.title('Cross Entropy Loss')\n    pyplot.plot(history.history['loss'], color='blue', label='train')\n    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    pyplot.subplot(212)\n    pyplot.title('Classification Accuracy')\n    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n    # save plot to file\n    #filename = sys.argv[0].split('/')[-1]\n    pyplot.savefig(filename + '_plot.png')\n    pyplot.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data augmentation involves making copies of the examples in the training dataset with small random modifications.\n\nThis has a regularizing effect as it both expands the training dataset and allows the model to learn the same general features, although in a more generalized manner.\n\nThere are many types of data augmentation that could be applied. Given that the dataset is comprised of small photos of objects, we do not want to use augmentation that distorts the images too much, so that useful features in the images can be preserved and used.\n\nThe types of random augmentations that could be useful include a horizontal flip, minor shifts of the image, and perhaps small zooming or cropping of the image.\n\nWe will investigate the effect of simple augmentation on the baseline image, specifically horizontal flips and 10% shifts in the height and width of the image.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the test harness for evaluating a model\ndef run_test_harness(trainX, trainY, testX, testY, iteration):\n\t# prepare pixel data\n\ttrainX, testX = prep_pixels(trainX, testX)\n\t# define model\n\tmodel = define_model()\n\t# create data generator\n\tdatagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n\t# prepare iterator\n\tit_train = datagen.flow(trainX, trainY, batch_size=50)\n\t# fit model\n\tsteps = int(trainX.shape[0] / 50)\n\thistory = model.fit_generator(it_train, steps_per_epoch=steps, epochs=100, validation_data=(testX, testY), verbose=1)\n\t# evaluate model\n\t_, acc = model.evaluate(testX, testY, verbose=0)\n\tprint('> %.3f' % (acc * 100.0))\n\tmodel.summary()\n    # learning curves\n\tsummarize_diagnostics(history, iteration) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define cnn model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.2))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.4))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n    opt = SGD(lr=0.001, momentum=0.9)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_test_harness(trainX, trainY, testX, testY, '4_augment_6layer_drop_norm_')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reviewing the learning curves, we see a similar improvement in model performances as we do with dropout, although the plot of loss suggests that model performance on the test set may have stalled slightly sooner than it did with dropout.\n\nThe results suggest that perhaps a configuration that used both dropout and data augmentation might be effective.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}